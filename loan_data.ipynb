{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score\n",
    "from category_encoders import TargetEncoder\n",
    "from imblearn.combine import SMOTETomek\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from skopt import BayesSearchCV\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset (replace with your file path)\n",
    "df = pd.read_csv('loan_data.csv')\n",
    "\n",
    "# --- Preprocessing ---\n",
    "\n",
    "# Date handling\n",
    "df['DATE_OF_BIRTH'] = pd.to_datetime(df['DATE_OF_BIRTH'], format='%d-%m-%Y')\n",
    "df['DISBURSAL_DATE'] = pd.to_datetime(df['DISBURSAL_DATE'], format='%d-%m-%Y')\n",
    "df['AGE_AT_DISBURSAL'] = (df['DISBURSAL_DATE'] - df['DATE_OF_BIRTH']).dt.days / 365.25\n",
    "\n",
    "# Parse text to numerical\n",
    "def parse_years_months(val):\n",
    "    if pd.isna(val):\n",
    "        return 0\n",
    "    parts = val.split()\n",
    "    years = int(parts[0].replace('yrs', ''))\n",
    "    months = int(parts[1].replace('mon', ''))\n",
    "    return years * 12 + months\n",
    "\n",
    "df['AVERAGE_ACCT_AGE_MONTHS'] = df['AVERAGE_ACCT_AGE'].apply(parse_years_months)\n",
    "df['CREDIT_HISTORY_LENGTH_MONTHS'] = df['CREDIT_HISTORY_LENGTH'].apply(parse_years_months)\n",
    "\n",
    "# Handle missing values\n",
    "df['EMPLOYMENT_TYPE'] = df['EMPLOYMENT_TYPE'].fillna('Unknown')\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Binary feature\n",
    "df['HAS_BUREAU_HISTORY'] = (df['PERFORM_CNS_SCORE'] > 0).astype(int)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "df['PRI_SANCTION_DISBURSED_DIFF'] = df['PRI_SANCTIONED_AMOUNT'] - df['PRI_DISBURSED_AMOUNT']\n",
    "df['SEC_SANCTION_DISBURSED_DIFF'] = df['SEC_SANCTIONED_AMOUNT'] - df['SEC_DISBURSED_AMOUNT']\n",
    "df['PRI_OVERDUE_ACCTS_RATIO'] = df['PRI_OVERDUE_ACCTS'] / (df['PRI_NO_OF_ACCTS'] + 1e-5)\n",
    "df['PRI_DEBT_BURDEN'] = df['PRI_CURRENT_BALANCE'] / (df['PRI_DISBURSED_AMOUNT'] + 1e-5)\n",
    "df['RECENT_DELINQUENCY_RATE'] = df['DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS'] / (df['NEW_ACCTS_IN_LAST_SIX_MONTHS'] + 1e-5)\n",
    "df['HAS_OVERDUE_ACCTS'] = ((df['PRI_OVERDUE_ACCTS'] > 0) | (df['SEC_OVERDUE_ACCTS'] > 0)).astype(int)\n",
    "\n",
    "# --- Encoding ---\n",
    "\n",
    "encoder = TargetEncoder(cols=['BRANCH_ID', 'SUPPLIER_ID', 'MANUFACTURER_ID', 'CURRENT_PINCODE_ID', 'STATE_ID', 'EMPLOYEE_CODE_ID'])\n",
    "df_encoded = encoder.fit_transform(df, df['LOAN_DEFAULT'])\n",
    "employment_dummies = pd.get_dummies(df_encoded['EMPLOYMENT_TYPE'], prefix='EMPLOYMENT')\n",
    "df_encoded = pd.concat([df_encoded, employment_dummies], axis=1)\n",
    "\n",
    "# Feature list\n",
    "features = [\n",
    "    'DISBURSED_AMOUNT', 'ASSET_COST', 'LTV', 'AGE_AT_DISBURSAL',\n",
    "    'PERFORM_CNS_SCORE', 'HAS_BUREAU_HISTORY', 'PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS',\n",
    "    'PRI_OVERDUE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT',\n",
    "    'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE',\n",
    "    'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT',\n",
    "    'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS',\n",
    "    'AVERAGE_ACCT_AGE_MONTHS', 'CREDIT_HISTORY_LENGTH_MONTHS', 'NO_OF_INQUIRIES',\n",
    "    'PRI_SANCTION_DISBURSED_DIFF', 'SEC_SANCTION_DISBURSED_DIFF', 'PRI_OVERDUE_ACCTS_RATIO',\n",
    "    'PRI_DEBT_BURDEN', 'RECENT_DELINQUENCY_RATE', 'HAS_OVERDUE_ACCTS',\n",
    "    'BRANCH_ID', 'SUPPLIER_ID', 'MANUFACTURER_ID', 'CURRENT_PINCODE_ID', 'STATE_ID', 'EMPLOYEE_CODE_ID',\n",
    "    'EMPLOYMENT_Salaried', 'EMPLOYMENT_Self employed', 'EMPLOYMENT_Unknown'\n",
    "]\n",
    "\n",
    "X = df_encoded[features]\n",
    "y = df_encoded['LOAN_DEFAULT']\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- Handle Imbalance ---\n",
    "smt = SMOTETomek(random_state=42)\n",
    "X_train_res, y_train_res = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "# --- Scale Features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_res = scaler.fit_transform(X_train_res)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# --- Base Models ---\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier(random_state=42, eval_metric='auc')\n",
    "xgb_params = {'n_estimators': (100, 500), 'max_depth': (3, 10), 'learning_rate': (0.01, 0.3),\n",
    "              'subsample': (0.6, 1.0), 'colsample_bytree': (0.6, 1.0)}\n",
    "xgb_opt = BayesSearchCV(xgb, xgb_params, n_iter=20, cv=StratifiedKFold(5), scoring='roc_auc', random_state=42)\n",
    "xgb_opt.fit(X_train_res, y_train_res)\n",
    "xgb_best = xgb_opt.best_estimator_\n",
    "\n",
    "# LightGBM\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "lgb_params = {'n_estimators': (100, 500), 'num_leaves': (20, 100), 'learning_rate': (0.01, 0.3),\n",
    "              'bagging_fraction': (0.6, 1.0), 'feature_fraction': (0.6, 1.0)}\n",
    "lgb_opt = BayesSearchCV(lgb, lgb_params, n_iter=20, cv=StratifiedKFold(5), scoring='roc_auc', random_state=42)\n",
    "lgb_opt.fit(X_train_res, y_train_res)\n",
    "lgb_best = lgb_opt.best_estimator_\n",
    "\n",
    "# Neural Network\n",
    "def create_nn():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_res.shape[1]))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "nn = create_nn()\n",
    "nn.fit(X_train_res, y_train_res, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# --- Stacking ---\n",
    "\n",
    "# Generate base model predictions\n",
    "xgb_pred_train = xgb_best.predict_proba(X_train_res)[:, 1]\n",
    "lgb_pred_train = lgb_best.predict_proba(X_train_res)[:, 1]\n",
    "nn_pred_train = nn.predict(X_train_res).flatten()\n",
    "\n",
    "xgb_pred_test = xgb_best.predict_proba(X_test)[:, 1]\n",
    "lgb_pred_test = lgb_best.predict_proba(X_test)[:, 1]\n",
    "nn_pred_test = nn.predict(X_test).flatten()\n",
    "\n",
    "# Stack predictions\n",
    "stacked_train = np.column_stack((xgb_pred_train, lgb_pred_train, nn_pred_train))\n",
    "stacked_test = np.column_stack((xgb_pred_test, lgb_pred_test, nn_pred_test))\n",
    "\n",
    "# Meta-learner\n",
    "meta_learner = LogisticRegression()\n",
    "meta_learner.fit(stacked_train, y_train_res)\n",
    "final_pred_proba = meta_learner.predict_proba(stacked_test)[:, 1]\n",
    "final_pred = (final_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# --- Evaluation ---\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, final_pred_proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, final_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "f1 = f1_score(y_test, final_pred)\n",
    "\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# --- Interpretability ---\n",
    "\n",
    "explainer = shap.KernelExplainer(xgb_best.predict_proba, X_test[:100])  # Subset for speed\n",
    "shap_values = explainer.shap_values(X_test[:100])\n",
    "shap.summary_plot(shap_values[1], X_test[:100], feature_names=features)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
